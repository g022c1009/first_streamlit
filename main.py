import streamlit as st
import requests
from bs4 import BeautifulSoup
import openai
import time
from cachetools import TTLCache
import os

openai.api_key = openai.api_key = os.getenv('OPENAI_API_KEY')  # APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š: URLã”ã¨ã«æœ€å¤§100ä»¶ã€1æ™‚é–“ã®TTL
cache = TTLCache(maxsize=100, ttl=3600)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®è¨­å®š: 1åˆ†é–“ã«æœ€å¤§60å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
RATE_LIMIT = 60
request_times = []

def is_rate_limited():
    current_time = time.time()
    # 1åˆ†å‰ã®æ™‚é–“ã‚’è¨ˆç®—
    one_minute_ago = current_time - 60
    # 1åˆ†ä»¥å†…ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã¿ã‚’ä¿æŒ
    while request_times and request_times[0] < one_minute_ago:
        request_times.pop(0)
    return len(request_times) >= RATE_LIMIT

def scrape_article(url):
    if url in cache:
        return cache[url]

    if is_rate_limited():
        st.error("ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ã‚’è¶…ãˆã¾ã—ãŸã€‚1åˆ†é–“ãŠå¾…ã¡ãã ã•ã„ã€‚")
        return None

    response = requests.get(url)
    if response.status_code != 200:
        st.error(f"è¨˜äº‹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')
    article_text = ""
    for p in soup.find_all('p'):
        article_text += p.get_text()

    cache[url] = article_text
    request_times.append(time.time())
    return article_text

def summarize_text(text):
    max_tokens = "150"
    prompt = f"ä»¥ä¸‹ã®è¨˜äº‹ã‚’{max_tokens}æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n{text}"
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",  # GPT-3.5 Turboã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯è¨˜äº‹ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content

if __name__ == "__main__":
    st.set_page_config(page_title="è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª", page_icon="ğŸ“°", layout="wide")

    st.title("è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª")
    st.write("è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ãƒˆ: [Bloomberg](https://www.bloomberg.co.jp/)")
    
    url = st.text_input("è¨˜äº‹ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    if st.button("è¨˜äº‹ã‚’è¦ç´„ã™ã‚‹"):
        if url:
            my_bar = st.progress(0)
            article_text = scrape_article(url)
            my_bar.progress(30)
            if article_text:
                article_summary = summarize_text(article_text)
                my_bar.progress(60)
                st.subheader("è¨˜äº‹ã®è¦ç´„:")
                st.write(article_summary)
                my_bar.progress(100)
                time.sleep(1)
                my_bar.empty()
        else:
            st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.sidebar.title("æƒ…å ±")
    st.sidebar.info(
        "ã“ã®ã‚¢ãƒ—ãƒªã¯æ§˜ã€…ãªã‚µã‚¤ãƒˆã‹ã‚‰è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€chatGPTã‚’ä½¿ã£ã¦è¦ç´„ã—ã¾ã™ã€‚\n"
        "URLã‚’å…¥åŠ›ã—ã€ã€Œè¨˜äº‹ã‚’è¦ç´„ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n"
        "ã¾ãŸã€è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹å‰ã«ã€ãã®ã‚µã‚¤ãƒˆãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è¨±å¯ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    )
